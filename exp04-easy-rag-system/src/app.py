import streamlit as st
import time
import os

# --- ç¯å¢ƒå˜é‡è®¾ç½® ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)             

if "HF_TOKEN" in st.secrets:
    os.environ["HF_TOKEN"] = st.secrets["HF_TOKEN"]
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = os.path.join(project_root, 'hf_cache')

# --- å¯¼å…¥æ¨¡å— ---
from config import (
    DATA_FILE, EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, TOP_K,
    MAX_ARTICLES_TO_INDEX, MILVUS_LITE_DATA_PATH, COLLECTION_NAME,
    id_to_doc_map
)
from data_utils import load_data
from models import load_embedding_model, load_generation_model
from milvus_utils import get_milvus_client, setup_milvus_collection, index_data_if_needed, search_similar_documents
from rag_core import generate_answer, load_reranker_model, rerank_documents 

# --- Streamlit é¡µé¢é…ç½® ---
st.set_page_config(page_title="åŒ»ç–— RAG åŠ©æ‰‹", layout="wide")

# --- ä¾§è¾¹æ ï¼šç³»ç»ŸçŠ¶æ€ä¸æ§åˆ¶ ---
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿæ§åˆ¶")
    
    # æ¸…é™¤å†å²æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.divider()
# æ ¸å¿ƒä¿¡æ¯ç›´æ¥æ˜¾ç¤º
    st.markdown("### æ ¸å¿ƒé…ç½®")
    st.info(f"**LLM:** {GENERATION_MODEL_NAME}")
    st.info(f"**Embedding:** {EMBEDDING_MODEL_NAME}")
    
    # è¯¦ç»†å‚æ•°æ”¾å…¥æŠ˜å é¢æ¿ï¼Œç‚¹å‡»æ‰ä¼šå±•å¼€
    with st.expander("ğŸ“Š æŸ¥çœ‹è¯¦ç»†å‚æ•°", expanded=False):
        st.markdown(f"**çŸ¥è¯†åº“é›†åˆ:** `{COLLECTION_NAME}`")
        st.markdown(f"**å‘é‡åº“è·¯å¾„:** `{MILVUS_LITE_DATA_PATH}`") 
        st.markdown(f"**æ•°æ®æºæ–‡ä»¶:** `{os.path.basename(DATA_FILE)}`")
        st.markdown("---")
        st.markdown(f"**æœ€å¤§ç´¢å¼•æ–‡æ¡£:** `{MAX_ARTICLES_TO_INDEX}`")
        st.markdown(f"**æ£€ç´¢ Top-K:** `{TOP_K}`")             

# --- ä¸»æ ‡é¢˜ ---
st.title("ğŸ©º æ™ºèƒ½åŒ»ç–—é—®ç­”ç³»ç»Ÿ")
st.caption(f"åŸºäº Milvus Lite + {GENERATION_MODEL_NAME} + BGE-Reranker æ„å»º")

# --- æ ¸å¿ƒåˆå§‹åŒ–é€»è¾‘ (ä½¿ç”¨ st.status ç¾åŒ–åŠ è½½è¿‡ç¨‹) ---
if "init_done" not in st.session_state:
    st.session_state.init_done = False

# åˆå§‹åŒ– Session State ç”¨äºå­˜å‚¨å¯¹è¯
if "messages" not in st.session_state:
    st.session_state.messages = []

# å®šä¹‰å…¨å±€å˜é‡å ä½ç¬¦
milvus_client = None
embedding_model = None
generation_model = None
tokenizer = None
reranker_model = None

with st.status("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿæ ¸å¿ƒç»„ä»¶...", expanded=not st.session_state.init_done) as status:
    # 1. åˆå§‹åŒ– Milvus
    st.write("ğŸ”Œ è¿æ¥å‘é‡æ•°æ®åº“ (Milvus Lite)...")
    milvus_client = get_milvus_client()
    
    if milvus_client:
        setup_milvus_collection(milvus_client)
        
        # 2. åŠ è½½æ¨¡å‹
        st.write("ğŸ§  åŠ è½½ Embedding æ¨¡å‹...")
        embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
        
        st.write("ğŸš€ åŠ è½½ç”Ÿæˆæ¨¡å‹ (LLM)...")
        generation_model, tokenizer = load_generation_model(GENERATION_MODEL_NAME)
        
        st.write("âš–ï¸ åŠ è½½ Re-ranker é‡æ’åºæ¨¡å‹...")
        reranker_model = load_reranker_model("BAAI/bge-reranker-base")
        
        # 3. å¤„ç†æ•°æ®
        st.write("ğŸ“š æ£€æŸ¥å¹¶ç´¢å¼•çŸ¥è¯†åº“...")
        pubmed_data = load_data(DATA_FILE)
        if pubmed_data and embedding_model:
            index_data_if_needed(milvus_client, pubmed_data, embedding_model)
        
        st.session_state.init_done = True
        status.update(label="âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼", state="complete", expanded=False)
    else:
        status.update(label="âŒ Milvus åˆå§‹åŒ–å¤±è´¥", state="error")
        st.stop()

# --- èŠå¤©ç•Œé¢é€»è¾‘ ---

# [cite_start]1.ä¸ä»…æ˜¾ç¤ºå†å²æ¶ˆæ¯ï¼Œè¿˜è¦ç¡®ä¿æ¯æ¬¡ rerun éƒ½æ¸²æŸ“å‡ºæ¥ [cite: 26]
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 2. æ¥æ”¶ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥å…³äºè¡€æ¶²ç–¾ç—…çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ç”ŸæˆåŠ©æ‰‹å›å¤
    with st.chat_message("assistant"):
        # åˆ›å»ºå ä½ç¬¦ï¼Œç”¨äºåŠ¨æ€æ›´æ–°çŠ¶æ€
        status_placeholder = st.empty()
        response_placeholder = st.empty()
        
        try:
            # A. æ£€ç´¢ (Retrieval)
            status_placeholder.markdown("ğŸ” *æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...*")
            start_time = time.time()
            
            # åˆç­› Top-K (å»ºè®® Config ä¸­è®¾ä¸º 10 æˆ– 20ï¼Œç»™ Rerank ç•™ç©ºé—´)
            retrieved_ids, distances = search_similar_documents(milvus_client, prompt, embedding_model)
            
            if not retrieved_ids:
                full_response = "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
                final_docs = []
            else:
                # æ˜ å°„ ID åˆ°æ–‡æœ¬
                initial_docs = [id_to_doc_map[doc_id] for doc_id in retrieved_ids if doc_id in id_to_doc_map]
                
                # B. é‡æ’åº (Re-ranking)
                if reranker_model:
                    status_placeholder.markdown("âš–ï¸ *æ­£åœ¨è¿›è¡Œè¯­ä¹‰é‡æ’åº...*")
                    # å–é‡æ’åºåçš„ Top-3
                    final_docs = rerank_documents(prompt, initial_docs, reranker_model, top_k=3)
                else:
                    final_docs = initial_docs[:3] # é™çº§å¤„ç†

                # C. ç”Ÿæˆ (Generation)
                status_placeholder.markdown("âœï¸ *æ­£åœ¨ç”Ÿæˆå›ç­”...*")
                answer = generate_answer(prompt, final_docs, generation_model, tokenizer)
                
                # è®¡ç®—è€—æ—¶
                cost_time = time.time() - start_time
                
                # æ‹¼æ¥æœ€ç»ˆå›å¤ (åŒ…å«å¼•ç”¨æº)
                full_response = answer + "\n\n---\n**å‚è€ƒæ–‡æ¡£:**"
                for idx, doc in enumerate(final_docs):
                    score_info = f"(Score: {doc.get('rerank_score', 0):.2f})" if 'rerank_score' in doc else ""
                    full_response += f"\n{idx+1}. **{doc['title']}** {score_info}"
                
                full_response += f"\n\n*(è€—æ—¶: {cost_time:.2f}s)*"

            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            status_placeholder.empty() # æ¸…é™¤çŠ¶æ€æç¤º
            response_placeholder.markdown(full_response)
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")