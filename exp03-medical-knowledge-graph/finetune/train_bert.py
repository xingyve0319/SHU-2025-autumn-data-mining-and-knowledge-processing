import sys
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForTokenClassification, 
    TrainingArguments, 
    Trainer,
    DataCollatorForTokenClassification
)

# -------------------------
# 1. è·¯å¾„è®¾ç½® (å…³é”®æ­¥éª¤)
# -------------------------
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° python pathï¼Œè¿™æ ·æ‰èƒ½å¯¼å…¥ src åŒ…
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

# å¯¼å…¥ä½ çš„è‡ªå®šä¹‰å·¥å…·
from src.utils.setup import set_hf_mirrors, set_seed, setup_logging

# -------------------------
# 2. å…¨å±€é…ç½®
# -------------------------
# æŒ‡å®šåªç”¨ä¸€å¼ å¡è®­ç»ƒï¼ˆBERT-Base åœ¨å•å¼  2080Ti ä¸Šè·‘å¾—å¾ˆå¿«ï¼Œå¤šå¡é…ç½®å¤æ‚ä¸”æ”¶ç›Šä¸é«˜ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "0" 

MODEL_NAME = "google-bert/bert-base-chinese"
OUTPUT_DIR = os.path.join(project_root, "models", "my_medical_bert") # ç»å¯¹è·¯å¾„æ›´å®‰å…¨

# æ ‡ç­¾å®šä¹‰ (å¿…é¡»ä¸ prepare_data.py ä¸€è‡´)
LABEL_LIST = [
    "O", 
    "B-disease", "I-disease", 
    "B-symptom", "I-symptom", 
    "B-drug", "I-drug", 
    "B-check", "I-check"
]
id2label = {i: label for i, label in enumerate(LABEL_LIST)}
label2id = {label: i for i, label in enumerate(LABEL_LIST)}

def main():
    # -------------------------
    # 3. åˆå§‹åŒ–ç¯å¢ƒ
    # -------------------------
    # è®¾ç½®é•œåƒ
    set_hf_mirrors()
    # å›ºå®šéšæœºç§å­
    set_seed(42)
    # è®¾ç½®æ—¥å¿—
    log_dir = os.path.join(project_root, "logs")
    logger = setup_logging(save_dir=log_dir, model_name="bert_finetune")
    
    logger.info(f"ğŸš€ å¼€å§‹å¾®è°ƒä»»åŠ¡")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    logger.info(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    # -------------------------
    # 4. åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
    # -------------------------
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    model = AutoModelForTokenClassification.from_pretrained(
        MODEL_NAME, 
        num_labels=len(LABEL_LIST),
        id2label=id2label,
        label2id=label2id
    )

    # -------------------------
    # 5. æ•°æ®å¤„ç†
    # -------------------------
    # å®šä¹‰å¤„ç†å‡½æ•° (æ”¾åœ¨è¿™é‡Œæ˜¯ä¸ºäº†èƒ½ç›´æ¥ä½¿ç”¨ tokenizer å˜é‡)
    def tokenize_and_align_labels(examples):
        tokenized_inputs = tokenizer(
            examples["tokens"], 
            truncation=True, 
            is_split_into_words=True, 
            max_length=512
        )

        labels = []
        for i, label in enumerate(examples["ner_tags"]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            previous_word_idx = None
            label_ids = []
            for word_idx in word_ids:
                if word_idx is None:
                    label_ids.append(-100) # å¿½ç•¥ç‰¹æ®Š token (CLS, SEP)
                elif word_idx != previous_word_idx:
                    label_ids.append(label2id.get(label[word_idx], 0)) # é»˜è®¤ä¸ºO
                else:
                    # å¯¹äºä¸­æ–‡ï¼ŒåŒä¸€ä¸ªè¯çš„åç»­subwordä¹Ÿæ ‡è®°ä¸ºç›¸åŒæ ‡ç­¾
                    label_ids.append(label2id.get(label[word_idx], 0))
                previous_word_idx = word_idx
            labels.append(label_ids)

        tokenized_inputs["labels"] = labels
        return tokenized_inputs

    # åŠ è½½æ•°æ® (ä½¿ç”¨ç»å¯¹è·¯å¾„é¿å…æ‰¾ä¸åˆ°æ–‡ä»¶)
    data_dir = os.path.join(project_root, "data", "training_data")
    train_file = os.path.join(data_dir, "train.json")
    test_file = os.path.join(data_dir, "test.json")
    
    logger.info(f"åŠ è½½è®­ç»ƒæ•°æ®: {train_file}")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(train_file):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®: {train_file}ã€‚è¯·å…ˆè¿è¡Œ prepare_data.py")

    raw_train_dataset = load_dataset('json', data_files=train_file, split='train')
    raw_eval_dataset = load_dataset('json', data_files=test_file, split='train')
    
    logger.info("æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
    tokenized_train = raw_train_dataset.map(tokenize_and_align_labels, batched=True)
    tokenized_eval = raw_eval_dataset.map(tokenize_and_align_labels, batched=True)

    # -------------------------
    # 6. è®­ç»ƒé…ç½®
    # -------------------------
    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="steps",   
        eval_steps=100,           # æ¯100æ­¥éªŒè¯ä¸€æ¬¡
        save_strategy="steps",    # ä¿æŒ steps
        save_steps=100,           # æ¯100æ­¥ä¿å­˜ä¸€æ¬¡
        learning_rate=2e-5,       
        per_device_train_batch_size=32, # 2080Ti 11G æ˜¾å­˜å……è£•ï¼Œå¯ä»¥ç›´æ¥å¼€32
        per_device_eval_batch_size=32,
        num_train_epochs=5,       
        weight_decay=0.01,
        logging_steps=50,
        load_best_model_at_end=True,
        save_total_limit=2,       # æœ€å¤šä¿ç•™2ä¸ªæ¨¡å‹checkpointï¼Œé˜²æ­¢ç¡¬ç›˜çˆ†æ»¡
        fp16=True,                # å¼€å¯æ··åˆç²¾åº¦ï¼Œé€Ÿåº¦å¿«ä¸”çœæ˜¾å­˜
        dataloader_num_workers=4, # åŠ é€Ÿæ•°æ®åŠ è½½
        report_to="wandb",
        run_name="medical-bert"
    )
    
    data_collator = DataCollatorForTokenClassification(tokenizer)
    
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # -------------------------
    # 7. å¼€å§‹è®­ç»ƒ
    # -------------------------
    logger.info("å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)

if __name__ == "__main__":
    main()